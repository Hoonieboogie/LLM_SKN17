{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bc5b6db",
   "metadata": {},
   "source": [
    "# Practice\n",
    "> Model I/Oë¶€í„° Memoryê¹Œì§€ ì—°ìŠµí•˜ê³  ë„˜ì–´ê°€ ë´…ì‹œë‹¤ğŸ¿ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a85846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9323bc81",
   "metadata": {},
   "source": [
    "### 1. Chainì„ ì´ìš©í•œ Simple LLM\n",
    "\n",
    "1. PromptTemplate - System message, User message\n",
    "2. LLM (model)\n",
    "3. OutputParser\n",
    "- Chain ~> ê°„ë‹¨í•œ ì§ˆì˜ë¥¼ ë³´ë‚´ ì‘ë‹µ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6b7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain langchain_openai langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58cbd0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PromptTemplate ìƒì„±\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "system_message = \"ë„ˆëŠ” ìœ„íŠ¸ìˆê²Œ ë‹¨ë‹µí•˜ëŠ” ì±—ë´‡ì´ì•¼. ì´ëŸ´ ë•ŒëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ?\\n{emotion}\"\n",
    "\n",
    "prompt_tpl = PromptTemplate(\n",
    "    template = system_message,\n",
    "    input_variables = ['emotion'],\n",
    ")\n",
    "\n",
    "user_message = 'ë‚´ ê¸°ë¶„ì´ ë„ˆë¬´ ì•ˆ ì¢‹ì•„.'\n",
    "prompt = prompt_tpl.format(emotion=user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c478c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LLM ëª¨ë¸ ìš”ì²­ì„ ìœ„í•œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "endpoint = HuggingFaceEndpoint(\n",
    "    repo_id='MLP-KTLim/llama-3-Korean-Bllossom-8B', # conversational íƒœê·¸ê°€ ìˆëŠ” ëª¨ë¸ë§Œ ê°€ëŠ¥í•¨\n",
    "    task='text-generation',\n",
    "    max_new_tokens=1024,\n",
    "    huggingfacehub_api_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "hf_model = ChatHuggingFace(\n",
    "    llm=endpoint,\n",
    "    verbos=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac631b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ì‘ë‹µ ë¬¸ìì—´ë§Œ ì¶œë ¥í•˜ê¸° ìœ„í•œ OutputParser ìƒì„±\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "string_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc9fa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ë¶„ì´ ì•ˆì¢‹ìœ¼ë‹ˆê¹Œ, ê¸°ë¶„ ì¢‹ì€ ìŒì‹ì„ ì¶”ì²œí•´ì¤„ê²Œ. í•˜ì§€ë§Œ, ê·¸ê²ƒë„ ì•ˆë˜ë©´, ê¸°ë¶„ì´ ì¢‹ì•„ì§€ëŠ” ìŒì•…ì„ í‹€ì–´ì¤„ê²Œ!\n"
     ]
    }
   ],
   "source": [
    "# 4. Chain ìƒì„± ë° ì§ˆì˜\n",
    "chain = prompt_tpl | hf_model | string_parser\n",
    "\n",
    "output = chain.invoke(input={'emotion':'ë‚˜ ê¸°ë¶„ì´ ì•ˆì¢‹ì•„.'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d81cb",
   "metadata": {},
   "source": [
    "### 2. ë‹¨ê³„ë³„ ChatBot\n",
    "> ë‚´ ì´ë¦„ì„ ì•Œë ¤ì£¼ê³ , ë‚´ ì´ë¦„ì´ ë­ëƒê³  ë¬¼ì–´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e236d4",
   "metadata": {},
   "source": [
    "1. ê·¸ëƒ¥ Chat\n",
    "- ChatOpenAI, HumanMessage ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48e5b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b49d2076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='ì•ˆë…• ë‚˜ëŠ” í›ˆì´ì•¼', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "query = input()\n",
    "\n",
    "message = HumanMessage(content=query)\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b722973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì•ˆë…•, í›ˆì´! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ. ì–´ë–»ê²Œ ì§€ë‚´?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 14, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'id': 'chatcmpl-CCLtSkhik0wN54yH8GNGZNNPuPbj8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c2ca71f6-799e-4585-a6e4-9d06a5c30908-0', usage_metadata={'input_tokens': 14, 'output_tokens': 17, 'total_tokens': 31, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(message.content)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "656d4420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•, í›ˆì´! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ. ì–´ë–»ê²Œ ì§€ë‚´?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c87d8",
   "metadata": {},
   "source": [
    "2. ì§ì ‘ ëŒ€í™” ë§¥ë½ ìœ ì§€\n",
    "- ChatOpenAI, HumanMessage, AIMessageë§Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec664852",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b30fbbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¹ì‹ ì˜ ì´ë¦„ì€ í›ˆì´ì…ë‹ˆë‹¤! ë§ë‚˜ìš”? ë” ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ ê²ƒì´ ìˆìœ¼ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "human_message = HumanMessage(content=query)\n",
    "\n",
    "messages.append(human_message)\n",
    "#print(len(messages))\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "model_message = model.invoke(messages) # AIMessage\n",
    "messages.append(model_message)\n",
    "print(model_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9164ba66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‚˜: ì•ˆë…• ë‚˜ëŠ” í›ˆì´ì•¼\n",
      "ë´‡: ì•ˆë…•í•˜ì„¸ìš”, í›ˆì´! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "ë‚˜: ë‚˜ë‘ ì´ì•¼ê¸° í• ë˜?\n",
      "ë´‡: ë¬¼ë¡ ì´ì£ ! ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆŒê¹Œìš”? ê¶ê¸ˆí•œ ê²ƒì´ë‚˜ í•˜ê³  ì‹¶ì€ ì´ì•¼ê¸°ê°€ ìˆìœ¼ë©´ í¸í•˜ê²Œ ë§í•´ ì£¼ì„¸ìš”.\n",
      "ë‚˜: ê·¼ë° ë‚´ ì´ë¦„ ë­ë¼ê³ ?\n",
      "ë´‡: ë‹¹ì‹ ì˜ ì´ë¦„ì€ í›ˆì´ì…ë‹ˆë‹¤! ë§ë‚˜ìš”? ë” ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ ê²ƒì´ ìˆìœ¼ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "for message in messages:\n",
    "    if type(message) == HumanMessage:\n",
    "        print(\"ë‚˜:\", message.content)\n",
    "    else:\n",
    "        print(\"ë´‡:\", message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8bdffe",
   "metadata": {},
   "source": [
    "3. Memoryë¡œ ëŒ€í™” ë§¥ë½ ìœ ì§€\n",
    "- ì•„ë˜ ë‚´ìš©ì„ ì‚¬ìš©\n",
    "    - langchain_openaiì˜ ChatOpenAI\n",
    "    - langchain_core.messagesì˜ í´ë˜ìŠ¤\n",
    "    - langchain_core.runnablesì˜ í´ë˜ìŠ¤\n",
    "    - langchain_core.promptsì˜ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1065c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_messages(self, messages):\n",
    "        self.messages.extend(messages)\n",
    "        \n",
    "    def clear(self):\n",
    "        self.messages = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'InMemoryHistory(messages={str(self.messages)})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76302011",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_by_session_id(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee3e66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template('ë„ˆëŠ” {skill}ì„ ì˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì•¼'),\n",
    "    MessagesPlaceholder(variable_name='history'),\n",
    "    HumanMessagePromptTemplate.from_template('{query}')\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_by_session_id,\n",
    "    input_messages_key='query',\n",
    "    history_messages_key='history'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51f3decf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['history', 'query', 'skill'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000196F9DF7EC0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['skill'], input_types={}, partial_variables={}, template='ë„ˆëŠ” {skill}ì„ ì˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì•¼'), additional_kwargs={}), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c5989ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í›ˆì´ë€ ì´ë¦„ì´ì‹œì£ ! ë§ë‚˜ìš”?\n"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "response = chain_with_history.invoke(\n",
    "    {'skill': 'ëŒ€í™”', 'query': query},\n",
    "    config={'configurable': {'session_id': 'ì¼ìƒëŒ€í™”'}}\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "137e5399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ì¼ìƒëŒ€í™”': InMemoryHistory(messages=[HumanMessage(content='ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?', additional_kwargs={}, response_metadata={}), AIMessage(content='ì£„ì†¡í•˜ì§€ë§Œ, ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œì§€ ëª»í•©ë‹ˆë‹¤. ì´ë¦„ì„ ì•Œë ¤ì£¼ì‹œë©´ ê¸°ì–µí•˜ê² ìŠµë‹ˆë‹¤!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 31, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'id': 'chatcmpl-CCHbQkPU85olvCBc1pYTOFPwNpJ1u', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--7990a087-7ae1-4ad8-b64d-b21157564100-0', usage_metadata={'input_tokens': 31, 'output_tokens': 22, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='ì•„ ë¯¸ì•ˆ ë‚´ ì´ë¦„ì€ í›ˆì´ì•¼', additional_kwargs={}, response_metadata={}), AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, í›ˆì´! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 71, 'total_tokens': 92, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'id': 'chatcmpl-CCHbtNHXEiUtLxyjzMrmIbvQK7cLP', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--27386d50-291b-4d9c-928d-67b34f793ab0-0', usage_metadata={'input_tokens': 71, 'output_tokens': 21, 'total_tokens': 92, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?', additional_kwargs={}, response_metadata={}), AIMessage(content='í›ˆì´ë€ ì´ë¦„ì´ì‹œì£ ! ë§ë‚˜ìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 106, 'total_tokens': 117, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'id': 'chatcmpl-CCHc6OpKPoydTSJ8Ab3kkR9ueX2ze', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5937d383-2631-4492-8ff8-7afa34749f01-0', usage_metadata={'input_tokens': 106, 'output_tokens': 11, 'total_tokens': 117, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f288992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‚˜: ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\n",
      "ë´‡: ì£„ì†¡í•˜ì§€ë§Œ, ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œì§€ ëª»í•©ë‹ˆë‹¤. ì´ë¦„ì„ ì•Œë ¤ì£¼ì‹œë©´ ê¸°ì–µí•˜ê² ìŠµë‹ˆë‹¤!\n",
      "ë‚˜: ì•„ ë¯¸ì•ˆ ë‚´ ì´ë¦„ì€ í›ˆì´ì•¼\n",
      "ë´‡: ì•ˆë…•í•˜ì„¸ìš”, í›ˆì´! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "ë‚˜: ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\n",
      "ë´‡: í›ˆì´ë€ ì´ë¦„ì´ì‹œì£ ! ë§ë‚˜ìš”?\n"
     ]
    }
   ],
   "source": [
    "for message in store['ì¼ìƒëŒ€í™”'].messages:\n",
    "    if type(message) == HumanMessage:\n",
    "        print(\"ë‚˜:\", message.content)\n",
    "    else:\n",
    "        print(\"ë´‡:\", message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
